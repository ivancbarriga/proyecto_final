{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación librerías\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import  roc_auc_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga de datos de archivo .csv\n",
    "dataTraining = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTraining.zip', encoding='UTF-8', index_col=0)\n",
    "dataTesting = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTesting.zip', encoding='UTF-8', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procesamiento \n",
    "#variable de interes\n",
    "dataTraining['genres'] = dataTraining['genres'].map(lambda x: eval(x))\n",
    "le = MultiLabelBinarizer()\n",
    "y_genres = le.fit_transform(dataTraining['genres'])\n",
    "\n",
    "# Se combina el título de la película con la trama\n",
    "dataTraining_2 = dataTraining.copy()\n",
    "dataTraining_2['title_plot'] = dataTraining_2['title'] + ' - ' + dataTraining_2['plot']\n",
    "dataTraining_2.drop(columns=['title','plot','rating'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "#Encoder\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()\n",
    "import tensorflow_hub as hub\n",
    "import seaborn as sns\n",
    "\n",
    "# Importación el módulo TF Hub del Universal Sentence Encoder\n",
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/2\"\n",
    "embed = hub.Module(module_url)\n",
    "\n",
    "# Codificación de las frases anteriormente definidas con la libreria tensorflow\n",
    "with tf.Session() as session:\n",
    "    session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "    sentences_embeddings = session.run(embed(dataTraining_2['title_plot']))\n",
    "\n",
    "#df con encoder y el indice de fila\n",
    "x_embed = pd.DataFrame(sentences_embeddings)\n",
    "x_embed.index = dataTraining_2.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PostProcesamiento\n",
    "#Se adiciona el año de la película a la matriz de embedding\n",
    "dataTraining_3 = pd.concat([dataTraining_2, x_embed], axis=1)\n",
    "dataTraining_3.drop(columns=['title_plot', 'genres'], inplace=True)\n",
    "\n",
    "# Se estandariza la data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "dataTraining_3 = scaler.fit_transform(dataTraining_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se separa la data en conjunto de entrenamiento y conjunto de validación\n",
    "X_train, X_test, y_train_genres, y_test_genres = train_test_split(dataTraining_3, y_genres, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2b820403d00>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importación de librerías para la implementación de la red neuronal\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.preprocessing import sequence\n",
    "from livelossplot import PlotLossesKeras\n",
    "from keras.callbacks import EarlyStopping\n",
    "%matplotlib inline\n",
    "\n",
    "dims = dataTraining_3.shape[1]\n",
    "var_out = y_genres.shape[1]\n",
    "\n",
    "K.clear_session()\n",
    "model = Sequential()  \n",
    "model.add(Dense(512, input_shape=(dims,), activation='sigmoid'))\n",
    "model.add(Dense(var_out, activation='sigmoid'))\n",
    "\n",
    "# Definición de función de perdida con parámetros definidos en la función nn_model_params\n",
    "model.compile(optimizer = 'adam', loss='binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Definición de la función EarlyStopping con parámetro definido en la función nn_model_params\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience = 10)\n",
    "\n",
    "# Entrenamiento de la red neuronal con parámetros definidos en la función nn_model_params\n",
    "model.fit(X_train, y_train_genres, \n",
    "          validation_data = (X_test, y_test_genres),\n",
    "          epochs=6,\n",
    "          batch_size=64,\n",
    "          callbacks=[early_stopping],\n",
    "          verbose=0 \n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'proy_final/scaler.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Exportar modelo a archivo binario .pkl\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mproy_final/scaler.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:477\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(value, filename, compress, protocol, cache_size)\u001b[0m\n\u001b[0;32m    471\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease do not set \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcache_size\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m in joblib.dump, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    472\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthis parameter has no effect and will be removed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    473\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou used \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcache_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(cache_size),\n\u001b[0;32m    474\u001b[0m                   \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compress_level \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 477\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_write_fileobject\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcompress_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mcompress_level\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    479\u001b[0m         NumpyPickler(f, protocol\u001b[38;5;241m=\u001b[39mprotocol)\u001b[38;5;241m.\u001b[39mdump(value)\n\u001b[0;32m    480\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_filename:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle_utils.py:200\u001b[0m, in \u001b[0;36m_write_fileobject\u001b[1;34m(filename, compress)\u001b[0m\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _buffered_write_file(file_instance)\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m     file_instance \u001b[38;5;241m=\u001b[39m \u001b[43m_COMPRESSORS\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mzlib\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompressor_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompresslevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompresslevel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _buffered_write_file(file_instance)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\compressor.py:107\u001b[0m, in \u001b[0;36mCompressorWrapper.compressor_file\u001b[1;34m(self, fileobj, compresslevel)\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfileobj_factory(fileobj, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfileobj_factory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfileobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mcompresslevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompresslevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\compressor.py:287\u001b[0m, in \u001b[0;36mBinaryZlibFile.__init__\u001b[1;34m(self, filename, mode, compresslevel)\u001b[0m\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid mode: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (mode,))\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filename, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 287\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp \u001b[38;5;241m=\u001b[39m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_closefp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'proy_final/scaler.pkl'"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "# Exportar modelo a archivo binario .pkl\n",
    "joblib.dump(scaler, 'proy_final/scaler.pkl', compress=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serializar el modelo a JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"proy_final/model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serializar los pesos a HDF5\n",
    "model.save_weights(\"proy_final/model.h5\")\n",
    "print(\"Modelo Guardado!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "# cargar json y crear el modelo\n",
    "json_file = open('proy_final/model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# cargar pesos al nuevo modelo\n",
    "loaded_model.load_weights(\"proy_final/model.h5\")\n",
    "print(\"Cargado modelo desde disco.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(optimizer = 'adam', loss='binary_crossentropy', metrics = ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicción del modelo de clasificación\n",
    "y_pred_genres = loaded_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Impresión del desempeño del modelo\n",
    "roc_auc_score(y_test_genres, y_pred_genres, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proy_final.proyecto_deployment import transformar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_datos = pd.DataFrame({'year': 1999, 'title': 'Message in a Bottle', 'plot': \"who meets by fate ,  shall be sealed by fate .  theresa osborne is running along the beach when she stumbles upon a bottle washed up on the shore .  inside is a message ,  reading the letter she feels so moved and yet she felt as if she has violated someone ' s thoughts .  in love with a man she has never met ,  theresa tracks down the author of the letter to a small town in wilmington ,  two lovers with crossed paths .  but yet one can ' t let go of their past .\"\n",
    "                        }, index=[0])\n",
    "df_datos\n",
    "\n",
    "a = transformar(df_datos)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tratamiento para nuevos datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para predecir en la base de datos de test de kaggle hay que hacer el mismo tratamiento:\n",
    "\n",
    "dataTesting_2 = dataTesting.copy()\n",
    "dataTesting_2['title_plot'] = dataTesting_2['title'] + ' - ' + dataTesting_2['plot']\n",
    "dataTesting_2.drop(columns=['title','plot'], inplace=True)\n",
    "\n",
    "tf.disable_eager_execution()\n",
    "\n",
    "# Importación el módulo TF Hub del Universal Sentence Encoder\n",
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/2\"\n",
    "embed = hub.Module(module_url)\n",
    "\n",
    "# Codificación de las frases anteriormente definidas con la libreria tensorflow\n",
    "with tf.Session() as session:\n",
    "    session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "    sentences_embeddings_2 = session.run(embed(dataTesting_2['title_plot']))\n",
    "\n",
    "x_test_embed = pd.DataFrame(sentences_embeddings_2)\n",
    "x_test_embed.index = dataTesting_2.index\n",
    "\n",
    "dataTesting_3 = pd.concat([dataTesting_2, x_test_embed], axis=1)\n",
    "dataTesting_3.drop(columns=['title_plot'], inplace=True)\n",
    "dataTesting_3 = scaler.transform(dataTesting_3)\n",
    "dataTesting_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicción nuevos datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_genres_2 = model.predict(dataTesting_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Guardar predicciones en formato exigido en la competencia de kaggle\n",
    "\n",
    "cols = ['p_Action', 'p_Adventure', 'p_Animation', 'p_Biography', 'p_Comedy', 'p_Crime', 'p_Documentary', 'p_Drama', 'p_Family',\n",
    "        'p_Fantasy', 'p_Film-Noir', 'p_History', 'p_Horror', 'p_Music', 'p_Musical', 'p_Mystery', 'p_News', 'p_Romance',\n",
    "        'p_Sci-Fi', 'p_Short', 'p_Sport', 'p_Thriller', 'p_War', 'p_Western']\n",
    "\n",
    "res = pd.DataFrame(y_pred_genres_2, index=dataTesting.index, columns=cols)\n",
    "res.to_csv('pred_calib_param_3.csv', index_label='ID')\n",
    "res.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nota: lo entreno otra vez uno diferente, para validar que la infraestructura del anterior no incide en el problema de guardado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "# Definición red neuronal con la función Sequential()\n",
    "model2 = Sequential()\n",
    "    \n",
    "# Definición de las capas de la red con el número de neuronas y la función de activación definidos en la función nn_model_params\n",
    "model2.add(Dense(512, input_shape=(dims,), activation='sigmoid'))\n",
    "model2.add(Dense(var_out, activation='sigmoid'))\n",
    "\n",
    "# Definición de función de perdida con parámetros definidos en la función nn_model_params\n",
    "model2.compile(optimizer = 'adam', loss='binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Definición de la función EarlyStopping con parámetro definido en la función nn_model_params\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience = 10)\n",
    "\n",
    "# Entrenamiento de la red neuronal con parámetros definidos en la función nn_model_params\n",
    "model2.fit(X_train, y_train_genres,\n",
    "          validation_data = (X_test, y_test_genres),\n",
    "          epochs=20,\n",
    "          batch_size=64,\n",
    "          callbacks=[early_stopping, PlotLossesKeras()],\n",
    "          verbose=True\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "import tensorflow as tf\n",
    "\n",
    "# guardar el modelo\n",
    "#joblib.dump(model, 'API/modelo_red_neuronal.pkl', compress=3)\n",
    "joblib.dump(scaler, 'model_deploy/scaler.pkl', compress=3)\n",
    "\n",
    "#tf.saved_model.save(model, 'API/modelo_red_neuronal3')\n",
    "\n",
    "model2.save('model_deploy/modelo_red_neuronal_h5.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "modelo_cargado = load_model('API/modelo_red_neuronal_h5.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_cargado.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTesting.loc[1]['plot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_datos = pd.DataFrame({'year': 1999, 'title': 'Message in a Bottle', 'plot': \"who meets by fate ,  shall be sealed by fate .  theresa osborne is running along the beach when she stumbles upon a bottle washed up on the shore .  inside is a message ,  reading the letter she feels so moved and yet she felt as if she has violated someone ' s thoughts .  in love with a man she has never met ,  theresa tracks down the author of the letter to a small town in wilmington ,  two lovers with crossed paths .  but yet one can ' t let go of their past .\"\n",
    "                        }, index=[0])\n",
    "df_datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from API.proyecto2_deployment import transformar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = transformar(df_datos)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask\n",
    "from flask_restx import Api, Resource, fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "api = Api(\n",
    "    app, \n",
    "    version='1.0', \n",
    "    title='Clasificación de género de películas',\n",
    "    description='Clasificación de género de películas')\n",
    "\n",
    "ns = api.namespace('predict', \n",
    "     description='Predicción géneros de la película')\n",
    "\n",
    "parser = api.parser()\n",
    "\n",
    "parser.add_argument(\n",
    "    'year', \n",
    "    type=int, \n",
    "    required=True, \n",
    "    help='Año del lanzamiento de la película', \n",
    "    location='args')\n",
    "\n",
    "parser.add_argument(\n",
    "    'title', \n",
    "    type=str, \n",
    "    required=True, \n",
    "    help='Nombre de la película', \n",
    "    location='args')\n",
    "\n",
    "parser.add_argument(\n",
    "    'plot', \n",
    "    type=str, \n",
    "    required=True, \n",
    "    help='Trama de la película', \n",
    "    location='args')\n",
    "\n",
    "resource_fields = api.model('Resource', {\n",
    "    'result': fields.String,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ns.route('/')\n",
    "class PrediccionApi(Resource):\n",
    "\n",
    "    @api.doc(parser=parser)\n",
    "    @api.marshal_with(resource_fields)\n",
    "    def get(self):\n",
    "        args = parser.parse_args()\n",
    "        \n",
    "        return {\n",
    "         \"result\": transformar(args)  \n",
    "        }, 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.run(debug=True, use_reloader=False, host='0.0.0.0', port=8888)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze '/requirem.txt'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
